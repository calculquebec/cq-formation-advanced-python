<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <title>Calcul Québec: Advanced and Parallel Python</title>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="css/swc.css" />
    <meta charset="UTF-8" />
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body class="lesson">
    <div class="container card">
      <div class="banner">
        <img style="height: 50px; margin-top: 20px;" src="img/logo-universite-laval.jpg" />
        <a href="http://www.calculquebec.ca" title="Calcul Québec"><img style="height: 70px; float: right; margin-top: 10px;" src="img/calculquebec_logo_small.jpg" /></a>
      </div>
      <article>
      <div class="row">
        <div class="col-md-10 col-md-offset-1">
                    <a href="index.html"><h1 class="title">Advanced and Parallel Python</h1></a>
          <h2 class="subtitle">Scaling Beyond One Machine</h2>
          <p>The technique used with the multiprocessing Python module is a fast way to achieve good scaling. Although it is based on a distributed-memory (processes) pattern, it doesn’t provide scaling beyond one physical machine. For that, you need something that provides transparent network communication between processes. There are a multitude of solutions for this kind of pattern when using Python but we’ll stick with an old, but proven, method called MPI.</p>
<h3 id="mpi">MPI</h3>
<p>MPI stands for <em>Message Passing Interface</em>. As its name implies, it is a library for passing messages around between processes. A message can be anything from a simple digit to a giant Numpy matix. It is important to note that it is built to handle message passing between processes that may or may not be on the same machine. It means this method can scale to hundreds or thousands of computing cores.</p>
<h3 id="how-does-it-work">How Does It Work?</h3>
<p>Since it is an interface, many libraries can implement it. We are going to focus on <a href="https://www.open-mpi.org/">OpenMPI</a> since it is open source and is widely available on many clusters.</p>
<p>Usually, when you run a Python program, you will use something like this:</p>
<pre class="input"><code>$ python my_program.py --argument1 ...</code></pre>
<p>Similarly, when you run an MPI-enabled program, you will use a launcher to start multiple instances (processes) of your program:</p>
<pre class="input"><code>$ mpirun python my_program.py --argument1 ...</code></pre>
<p>The default configuration on most machines, when using OpenMPI, is to use the same number of processes that there are cores available. It is possible to specify exactly the number of processes you want, for example if you are memory constrained. This would launch your program using 4 processes:</p>
<pre class="input"><code>$ mpirun -n 4 python my_program.py --argument1 ...</code></pre>
<p>If you try to run a non-MPI-enabled program using the mpirun launcher, you will have 4 instances doing exactly the same thing, not communicating with each other. When doing parallel computation, you want to split your input data, like in the multiprocessing example, and distribute it amongst all participant processes.</p>
<h3 id="mpi-enabled-python-program">MPI-enabled Python Program</h3>
<p>The most basic MPI-enabled program would look like this:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> mpi4py <span class="im">import</span> MPI

comm <span class="op">=</span> MPI.COMM_WORLD
rank <span class="op">=</span> comm.Get_rank()
size <span class="op">=</span> comm.Get_size()

<span class="bu">print</span> <span class="st">&quot;I am rank&quot;</span>, rank, <span class="st">&quot;of&quot;</span>, size</code></pre></div>
<p>We first import the MPI library from the mpi4py package. The <em>comm</em> variable stands for communicator, which is a fundamental part of the MPI programming paradigm: all communication between processes or groups of processes are sent using a communicator. You can see it as a pool of processes participating in a meeting. One communicator, called world, always exist and includes all processes.</p>
<p>The other interesting concept is the rank. Each process is assigned a unique number, ranging from 0 to <em>n</em>-1, where n is the total number of processes. This unique number is called a rank and is used to communicate with one process in particular.</p>
<p>Let’s try our first run of our MPI program:</p>
<pre class="input"><code>$ mpirun -np 4 python mpi.py</code></pre>
<pre class="output"><code>I am rank 3 of 4
I am rank 0 of 4
I am rank 1 of 4
I am rank 2 of 4</code></pre>
<p>As expected, we have 4 ranks, numbered 0 to 3. The interesting thing to notice is that the output is not sequential. This is the first thing to remember: those processes are really independent processes. They do their own thing, whenever they are ready, unless we synchronize them in some way, either explicitly or by adding communication between them.</p>
<p>The most important aspect of MPI programming is, as it was with the multiprocessing module, is to take care to split our processing equitably between processes. Since we will start with our multiprocessing solution from the last topic, we will use the same interval-splitting and go straight to communication.</p>
<p>Let’s have a look back at our previous (simplified) multiprocessing solution:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> sys
<span class="im">import</span> math
<span class="im">import</span> time

<span class="im">from</span> multiprocessing <span class="im">import</span> Pool

<span class="kw">def</span> approx_pi(intervals):
    pi <span class="op">=</span> <span class="fl">0.0</span>
    <span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(intervals[<span class="dv">0</span>], intervals[<span class="dv">1</span>]):
        pi <span class="op">+=</span> (<span class="dv">4</span> <span class="op">-</span> <span class="dv">8</span> <span class="op">*</span> (i <span class="op">%</span> <span class="dv">2</span>)) <span class="op">/</span> (<span class="bu">float</span>)(<span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span>)
    <span class="cf">return</span> pi

n <span class="op">=</span> <span class="bu">int</span>(sys.argv[<span class="dv">1</span>])
chunk_size <span class="op">=</span> n<span class="op">/</span><span class="dv">4</span>
intervals <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> p: [p<span class="op">*</span>chunk_size, p<span class="op">*</span>chunk_size<span class="op">+</span>chunk_size], <span class="bu">range</span>(<span class="dv">4</span>))
intervals[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="op">=</span> <span class="bu">max</span>(intervals[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>], n)

p <span class="op">=</span> Pool(<span class="dv">4</span>)
pi <span class="op">=</span> <span class="bu">sum</span>(p.<span class="bu">map</span>(approx_pi, intervals))</code></pre></div>
<h3 id="communication">Communication</h3>
<p>In MPI, there are two communication concepts:</p>
<ol style="list-style-type: decimal">
<li>Point to Point: used for exchanging data between two processes.</li>
<li>Collective Operations: used for exchanging data between any number of processes, in one operation.</li>
</ol>
<p>We will re-implement the previous example, in turn, using both communication patterns.</p>
<h4 id="point-to-point">Point to Point</h4>
<p>This mode of communication implies that one process can talk to only one other process at a time. One approach we could use in our implementation is to elect a process as our master process, and make it compute our intervals and send it to each other processes, one at a time. We will use our rank 0 for this task.</p>
<p>*** add note *** In most MPI program, rank 0 is doing more things like initialization or data distribution. Keep in mind that this is a design decision and that rank 0 is exactly the same as other processes. It just happens that rank 0 will always exist (since it’s the first process) so people tend to choose that one for this coordinating role.</p>
<p>The first thing we will add is the MPI library import:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> mpi4py <span class="im">import</span> MPI</code></pre></div>
<p>In our previous example, we had split our inputs into 4 intervals, assuming 4 processes. We want to become more generic here and plan for any number of processes:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    comm <span class="op">=</span> MPI.COMM_WORLD
    rank <span class="op">=</span> comm.Get_rank()
    size <span class="op">=</span> comm.Get_size()
    
    n <span class="op">=</span> <span class="bu">int</span>(sys.argv[<span class="dv">1</span>])
    chunk_size <span class="op">=</span> n<span class="op">/</span>size
    intervals <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> p: [p<span class="op">*</span>chunk_size, p<span class="op">*</span>chunk_size<span class="op">+</span>chunk_size], <span class="bu">range</span>(size))
    intervals[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="op">=</span> <span class="bu">max</span>(intervals[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>], n)</code></pre></div>
<p>Notice that we compute our chunk_size by dividing it by the number of ranks (processes) instead of 4.</p>
<p>We then go on with the intervals distribution to all ranks, using point to point communication:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        myInterval <span class="op">=</span> intervals[<span class="dv">0</span>]
        <span class="cf">for</span> otherRank <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, size):
            comm.send(intervals[otherRank], dest<span class="op">=</span>otherRank)
    <span class="cf">else</span>:
        myInterval <span class="op">=</span> comm.recv(source<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>At this point, each process has a myInterval variable defined as their own share of the data to process. myInterval on rank 0 would be [0, 25000000], on rank 1 [25000000, 50000000], and so on. Remember that we are still using processes, with a distributed memory model. That means every process has its own independent variable myInterval, with different values.</p>
<p>Notice the two point to point communication functions used:</p>
<ol style="list-style-type: decimal">
<li>comm.send: it is used to send, synchronously, data to a specific rank. Here we send the sub-list intervals[otherRank] to the rank otherRank.</li>
<li>comm.recv: it is used to receive, synchronously, data from a specific rank. Here we want to receive data from rank 0 (source=0).</li>
</ol>
<p>Note that those are blocking operations: it means if rank 0 forgets to send data to a rank, this rank will be blocked on the comm.recv call forever.</p>
<p>Once everyone has its share of the input data to process, here their own interval, we can process it using the approx_pi function, without modifying it:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    partial_pi <span class="op">=</span> approx_pi(myInterval)</code></pre></div>
<p>Once we have the partial sums, we need to get the result back to rank 0, and sum them all:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        pi <span class="op">=</span> partial_pi
        <span class="cf">for</span> otherRank <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, size):
            pi <span class="op">+=</span> comm.recv(source<span class="op">=</span>otherRank)
    <span class="cf">else</span>:
        comm.send(partial_pi, dest<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>One last thing to note: you usually want to output results, either on the terminal on to a file, only from one rank. So we will add the following to make sure only rank 0 prints the result:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        t2 <span class="op">=</span> time.time()
        <span class="bu">print</span>(<span class="st">&quot;PI is approximately </span><span class="sc">%.16f</span><span class="st">, Error is </span><span class="sc">%.16f</span><span class="st">&quot;</span><span class="op">%</span>(pi, <span class="bu">abs</span>(pi <span class="op">-</span> math.pi)))
        <span class="bu">print</span>(<span class="st">&quot;Time = </span><span class="sc">%.16f</span><span class="st"> sec</span><span class="ch">\n</span><span class="st">&quot;</span><span class="op">%</span>(t2 <span class="op">-</span> t1))</code></pre></div>
<p>Also remember that only rank 0 has done this summing so only rank 0 known the final pi value. Let’s recap everything at the same place:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> mpi4py <span class="im">import</span> MPI

<span class="im">import</span> sys
<span class="im">import</span> math
<span class="im">import</span> time

<span class="kw">def</span> approx_pi(intervals):
    pi <span class="op">=</span> <span class="fl">0.0</span>
    <span class="cf">for</span> i <span class="op">in</span> <span class="bu">range</span>(intervals[<span class="dv">0</span>], intervals[<span class="dv">1</span>]):
        pi <span class="op">+=</span> (<span class="dv">4</span> <span class="op">-</span> <span class="dv">8</span> <span class="op">*</span> (i <span class="op">%</span> <span class="dv">2</span>)) <span class="op">/</span> (<span class="bu">float</span>)(<span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span>)
    <span class="cf">return</span> pi

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:
    <span class="cf">if</span> <span class="bu">len</span>(sys.argv) <span class="op">!=</span> <span class="dv">2</span>:
        <span class="bu">print</span> <span class="op">&gt;&gt;</span> sys.stderr, <span class="st">&quot;usage: </span><span class="sc">{0}</span><span class="st"> &lt;intervals&gt;&quot;</span>.<span class="bu">format</span>(sys.argv[<span class="dv">0</span>])
        sys.exit(<span class="dv">1</span>)

    t1 <span class="op">=</span> time.time()

    comm <span class="op">=</span> MPI.COMM_WORLD
    rank <span class="op">=</span> comm.Get_rank()
    size <span class="op">=</span> comm.Get_size()
    
    n <span class="op">=</span> <span class="bu">int</span>(sys.argv[<span class="dv">1</span>])
    chunk_size <span class="op">=</span> n<span class="op">/</span>size
    intervals <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> p: [p<span class="op">*</span>chunk_size, p<span class="op">*</span>chunk_size<span class="op">+</span>chunk_size], <span class="bu">range</span>(size))
    intervals[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>] <span class="op">=</span> <span class="bu">max</span>(intervals[<span class="op">-</span><span class="dv">1</span>][<span class="dv">1</span>], n)

    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        myInterval <span class="op">=</span> intervals[<span class="dv">0</span>]
        <span class="cf">for</span> otherRank <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, size):
            comm.send(intervals[otherRank], dest<span class="op">=</span>otherRank)
    <span class="cf">else</span>:
        myInterval <span class="op">=</span> comm.recv(source<span class="op">=</span><span class="dv">0</span>)

    partial_pi <span class="op">=</span> approx_pi(myInterval)
    <span class="bu">print</span> <span class="st">&quot;Rank&quot;</span>, rank, <span class="st">&quot;partial pi:&quot;</span>, partial_pi

    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        pi <span class="op">=</span> partial_pi
        <span class="cf">for</span> otherRank <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, size):
            pi <span class="op">+=</span> comm.recv(source<span class="op">=</span>otherRank)
    <span class="cf">else</span>:
        comm.send(partial_pi, dest<span class="op">=</span><span class="dv">0</span>)

    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        t2 <span class="op">=</span> time.time()
        <span class="bu">print</span>(<span class="st">&quot;PI is approximately </span><span class="sc">%.16f</span><span class="st">, Error is </span><span class="sc">%.16f</span><span class="st">&quot;</span><span class="op">%</span>(pi, <span class="bu">abs</span>(pi <span class="op">-</span> math.pi)))
        <span class="bu">print</span>(<span class="st">&quot;Time = </span><span class="sc">%.16f</span><span class="st"> sec</span><span class="ch">\n</span><span class="st">&quot;</span><span class="op">%</span>(t2 <span class="op">-</span> t1))</code></pre></div>
<pre class="input"><code>$ mpirun -np 4 python approx_pi_mpi.py 100000000</code></pre>
<pre class="output"><code>Rank 3 partial pi: 3.33333333333e-09
Rank 0 partial pi: 3.14159261359
Rank 1 partial pi: 2.00000000006e-08
Rank 2 partial pi: 6.66666666771e-09
PI is approximately 3.1415926435898172, Error is 0.0000000099999760
Time = 9.0873570442199707 sec</code></pre>
<p>We get about the same run time as with our multiprocessing example, which is good. The code is arguably more complex, but keep in mind that now, we can scale beyond one single machine without changing a single line of code, which can be really useful, or even mandatory, to solve difficult problems.</p>
<h4 id="collective-operations">Collective Operations</h4>
<p>Collective operations imply that multiple processes coordinate for communicating. There are a handful of patterns that can be used:</p>
<ul>
<li>Broadcasting: used for sending the same data from one process to one or more processes.</li>
<li>Scattering: used for sending chunks of data to multiple processes.</li>
<li>Gathering: used for collecting chunks of data from multiple processes to one process.</li>
</ul>
<p>Our interval distribution sure looks like scattering so we’ll try to simplify our code a little using this collective operation. This is the sending part of our previous example:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        myInterval <span class="op">=</span> intervals[<span class="dv">0</span>]
        <span class="cf">for</span> otherRank <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, size):
            comm.send(intervals[otherRank], dest<span class="op">=</span>otherRank)
    <span class="cf">else</span>:
        myInterval <span class="op">=</span> comm.recv(source<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>It could be rewritten using a scatter collective operation like this:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    myInterval <span class="op">=</span> comm.scatter(intervals, root<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>The <em>intervals</em> list must have the same number of elements as there are processes in the communicator used, which is the case here.</p>
<p>At the end, the reverse operation looks like a gathering. We want to get all data back to rank 0 so it can sum the partial sums. This is the code we had previously:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        pi <span class="op">=</span> partial_pi
        <span class="cf">for</span> otherRank <span class="op">in</span> <span class="bu">range</span>(<span class="dv">1</span>, size):
            pi <span class="op">+=</span> comm.recv(source<span class="op">=</span>otherRank)
    <span class="cf">else</span>:
        comm.send(partial_pi, dest<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>Which can be replaced with the following:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    partial_sums <span class="op">=</span> comm.gather(partial_pi, root<span class="op">=</span><span class="dv">0</span>)
    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        pi <span class="op">=</span> <span class="bu">sum</span>(partial_sums)</code></pre></div>
<p>Note that partial_sums will be a list of all partial_pi on rank 0 and None on all other ranks.</p>
<h4 id="reduction">Reduction</h4>
<p>As we have seen previously, certain operations, like summing values from all processes, are called reductions. What we have done so far is called a manual reduction and might not be the most effective way to do it. Imagine having 10s of millions of elements to sum: would it be wise to send every single value to one single rank, and sum them while other processes just sit there and wait? Reduction, as implemented in most MPI libraries, have (hidden) strategies for such cases.</p>
<p>Let’s have a look at the way we do it now:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    partial_sums <span class="op">=</span> comm.gather(partial_pi, root<span class="op">=</span><span class="dv">0</span>)

    <span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:
        pi <span class="op">=</span> <span class="bu">sum</span>(partial_sums)</code></pre></div>
<p>This could be done efficiently, on many processes, using a reduction operation:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">    pi <span class="op">=</span> comm.<span class="bu">reduce</span>(partial_pi, op<span class="op">=</span>MPI.SUM, root<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>This statement basically means: sum all partial_pi variables and make the result available on rank 0. Running it yields the same result, in about the same run time.</p>
<pre class="input"><code>$ mpirun -np 4 python approx_pi_mpi.py 100000000</code></pre>
<pre class="output"><code>PI is approximately 3.1415926435898172, Error is 0.0000000099999760
Time = 8.9309309005737305 sec</code></pre>
<p>Note that, currently, the mpi4py implementation of the reduction operations are done naively and would do exactly what we did previously: gather everything on one rank and sum there. The advantage of using the reduction operation today is that the code is shorter, clearer, and that you will benefit from it once it is implemented in mpi4py without changing anything.</p>
        </div>
      </div>
      </article>
      <div class="footer">
      </div>
    </div>
    <!-- Javascript placed at the end of the document so the pages load faster -->
    <script src="http://software-carpentry.org/v5/js/jquery-1.9.1.min.js"></script>
    <script src="css/bootstrap/bootstrap-js/bootstrap.js"></script>
  </body>
</html>
